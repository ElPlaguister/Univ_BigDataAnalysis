{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58fccac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff5b483d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/03 21:30:19 WARN Utils: Your hostname, Kritiasui-MacBookAir.local resolves to a loopback address: 127.0.0.1; using 172.30.1.28 instead (on interface en0)\n",
      "21/10/03 21:30:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "21/10/03 21:30:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/10/03 21:30:21 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "myConf = pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession\\\n",
    ".builder\\\n",
    ".master('local')\\\n",
    ".appName('myApp')\\\n",
    ".config(conf=myConf)\\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cf8df4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_testList = [('key1', 1),('key1', 2),('key2', 3),('key1', 4),('key2', 5),\n",
    "            ('key1', 6),('key2', 7),\n",
    "            ('key1', 8),('key1', 9),('key2', 10),('key2', 11)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d105f960",
   "metadata": {},
   "source": [
    "partition이 1개이면 combiner, mergeValues만 작동"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "993af0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "_testRdd = spark.sparkContext.parallelize(_testList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8363c7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_testRdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "223b4b26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('key1', '1*#2#4#6#8#9'), ('key2', '3*#5#7#10#11')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_testRdd.combineByKey(lambda v : str(v) + '*', lambda c, v: c + '#' + str(v), lambda c1, c2 : c1 + '&' + c2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fc16af",
   "metadata": {},
   "source": [
    "partition이 복수인 경우에는 mergeCombiner가 작동한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4857f8c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_testRdd2 = spark.sparkContext.parallelize(_testList, 2)\n",
    "_testRdd2.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "615cbbf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions 0 -> [('key1', 1), ('key1', 2), ('key2', 3), ('key1', 4), ('key2', 5)]\n",
      "Partitions 1 -> [('key1', 6), ('key2', 7), ('key1', 8), ('key1', 9), ('key2', 10), ('key2', 11)]\n"
     ]
    }
   ],
   "source": [
    "partitions = _testRdd2.glom().collect()\n",
    "for num, partition in enumerate(partitions):\n",
    "    print(f'Partitions {num} -> {partition}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d5cf617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('key1', '1*#2#4&6*#8#9'), ('key2', '3*#5&7*#10#11')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_testRdd2.combineByKey(lambda v : str(v) + '*', lambda c, v: c + '#' + str(v), lambda c1, c2 : c1 + '&' + c2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05c86c32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('key1', (30, 6)), ('key2', (36, 5))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_testRdd2.combineByKey(lambda value: (value, 1), \n",
    "                       lambda x, value: (x[0] + value, x[1] + 1),\n",
    "                       lambda x, y: (x[0] + y[0], x[1] + y[1]))\\\n",
    ".collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "053dfcae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'key1': 5.0, 'key2': 7.2}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_testCbkRdd2 = _testRdd2.combineByKey(lambda value: (value, 1), \n",
    "                       lambda x, value: (x[0] + value, x[1] + 1),\n",
    "                       lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "averageByKey = _testCbkRdd2.map(lambda x: (x[0], x[1][0] / x[1][1]))\n",
    "averageByKey.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e07dd184",
   "metadata": {},
   "outputs": [],
   "source": [
    "marks = spark.sparkContext.parallelize([('kim', 86), ('lim', 87), ('kim', 75), ('kim', 91), ('lim', 78), ('lim', 92), ('lim', 79), ('lee', 99)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "001e7208",
   "metadata": {},
   "outputs": [],
   "source": [
    "marksByKey = marks.combineByKey(lambda value: (value, 1),\n",
    "                               lambda x, value: (x[0] + value, x[1] + 1),\n",
    "                               lambda x, y: (x[0] + y[0], x[1] + y[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9835d57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kim', (252, 3)), ('lim', (336, 4)), ('lee', (99, 1))]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marksByKey.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f98af6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "heights = spark.sparkContext.parallelize([\n",
    "    ('M', 182.), ('F', 164.), ('M', 180.), ('M', 185.), ('M', 171.), ('F', 162.)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d58856e",
   "metadata": {},
   "outputs": [],
   "source": [
    "heightsByKey = heights.combineByKey(lambda value : (value, 1),\n",
    "                                   lambda x, value : (x[0] + value, x[1] + 1),\n",
    "                                   lambda x, y: (x[0] + y[0], x[1] + y[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "38d2e7ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('M', (718.0, 4)), ('F', (326.0, 2))]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heightsByKey.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f5b066f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'M': 179.5, 'F': 163.0}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avgByKey = heightsByKey.map(lambda x: (x[0], x[1][0] / x[1][1]))\n",
    "avgByKey.collectAsMap()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
