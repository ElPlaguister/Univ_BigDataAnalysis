{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터프레임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/22 03:44:43 WARN Utils: Your hostname, Kritiasui-MacBookAir.local resolves to a loopback address: 127.0.0.1; using 172.30.1.23 instead (on interface en0)\n",
      "21/11/22 03:44:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "21/11/22 03:44:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/11/22 03:44:46 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "myConf = pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession\\\n",
    "    .builder\\\n",
    "    .master('local')\\\n",
    "    .appName('myApp')\\\n",
    "    .config(conf = myConf)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터프레임 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: long (nullable = true)\n",
      "\n",
      "[Row(_1='1', _2='kim, js', _3=170)]\n"
     ]
    }
   ],
   "source": [
    "myList = [('1', 'kim, js', 170),\n",
    "          ('1', 'lee, sm', 175),\n",
    "          ('2', 'lim, yg', 180),\n",
    "          ('2', 'lee', 170)]\n",
    "# 자동으로 Schema 설정됨\n",
    "myDf = spark.createDataFrame(myList)\n",
    "myDf.printSchema()\n",
    "print(myDf.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf = spark.createDataFrame(myList, ['year', 'name', 'height'])\n",
    "myDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Row객체를 사용해서 생성\n",
    "> Row : 이름이 붙여진 행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "Person = Row('year', 'name', 'height')\n",
    "row1 = Person('1', 'kim, js', 170)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row1: 1 kim, js 170\n"
     ]
    }
   ],
   "source": [
    "print(\"row1:\", row1.year, row1.name, row1.height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['year', 'name', 'height'])\n",
      "dict_values(['1', 'kim, js', 170])\n"
     ]
    }
   ],
   "source": [
    "row1_dict = row1.asDict()\n",
    "print(row1_dict.keys())\n",
    "print(row1_dict.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Row객체를 이용해서 데이터를 만들면, 별개의 Schema인수가 필요하지 않다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myRows = [Person(*x) for x in myList]\n",
    "myDf = spark.createDataFrame(myRows)\n",
    "myDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataType 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "mySchema = StructType([\n",
    "    StructField('year', StringType(), nullable = True),\n",
    "    StructField('name', StringType(), nullable = True),\n",
    "    StructField('height', IntegerType(), nullable = True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf = spark.createDataFrame(myRows, mySchema)\n",
    "myDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD에서 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# toDF를 이용한 생성\n",
    "myRdd = spark.sparkContext.parallelize(myList)\n",
    "myDfFromRdd = myRdd.toDF()\n",
    "myDfFromRdd.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# createDataFrame을 이용한 생성\n",
    "myDfFromRdd = spark.createDataFrame(myRdd)\n",
    "myDfFromRdd.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Row를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "_myRdd = myRdd.map(lambda x : Row(year = int(x[0]), name = x[1], height = int(x[2])))\n",
    "_myDf = spark.createDataFrame(_myRdd)\n",
    "_myDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='a', age=10), Row(name='b', age=20)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Row를 사용해서 RDD생성\n",
    "r1 = Row(name = 'a', age = 10)\n",
    "r2 = Row(name = 'b', age = 20)\n",
    "rRdd = spark.sparkContext.parallelize([r1, r2])\n",
    "rRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Row타입의 RDD를 DF로 생성\n",
    "rRdd.toDF().printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema를 완전히 정해서 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      "\n",
      "+---+----+------+\n",
      "| id|name|height|\n",
      "+---+----+------+\n",
      "|  1| kim|  50.0|\n",
      "|  2| lee|  60.0|\n",
      "|  3|park|  70.0|\n",
      "+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myRdd = spark.sparkContext.parallelize([(1, 'kim', 50.0), (2, 'lee', 60.0), (3, 'park', 70.0)])\n",
    "schema = StructType([\n",
    "    StructField('id', IntegerType(), True),\n",
    "    StructField('name', StringType(), True),\n",
    "    StructField('height', DoubleType(), True)\n",
    "])\n",
    "myDf = spark.createDataFrame(myRdd, schema)\n",
    "myDf.printSchema()\n",
    "myDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas에서 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>kim</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>lee</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>park</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  name  height\n",
       "0   1   kim    50.0\n",
       "1   2   lee    60.0\n",
       "2   3  park    70.0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDf.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "보류"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV, TSV에서 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cfile = os.path.join('data', 'spark_2cols.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD 통해서 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- col1: long (nullable = true)\n",
      " |-- col2: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lines = spark.sparkContext.textFile(cfile)\n",
    "_col12 = lines.map(lambda l : l.split(','))\n",
    "col12 = _col12.map(lambda p : Row(col1 = int(p[0].strip()), col2 = int(p[1].strip())))\n",
    "_myDf = spark.createDataFrame(col12)\n",
    "_myDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame 직접 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('com.databricks.spark.csv').options(header = 'false', inferschama = 'true', delimiter = ',')\\\n",
    "    .load(cfile)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- _c1: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.options(header = 'false', inferschema = 'true', delimiter = ',').csv(cfile)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tsv파일\n",
    "- delimiter를 \\t으로 설정해서 읽을 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON에서 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "fname = os.path.join('data', 'LandActualPriceInfo.json')\n",
    "with open(fname, 'rb') as f:\n",
    "    data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data_json_str = json.loads(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_json_str = b'[' + b','.join(data) + b']'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "infoPd = pd.read_json(data_json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- landActualPriceInfo: struct (nullable = true)\n",
      " |    |-- RESULT: struct (nullable = true)\n",
      " |    |    |-- CODE: string (nullable = true)\n",
      " |    |    |-- MESSAGE: string (nullable = true)\n",
      " |    |-- list_total_count: long (nullable = true)\n",
      " |    |-- row: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- ACC_YEAR: string (nullable = true)\n",
      " |    |    |    |-- BJDONG10_CD: string (nullable = true)\n",
      " |    |    |    |-- BJDONG_NM: string (nullable = true)\n",
      " |    |    |    |-- BLDG_AREA: string (nullable = true)\n",
      " |    |    |    |-- BLDG_MUSE_CD: string (nullable = true)\n",
      " |    |    |    |-- BLDG_MUSE_NM: string (nullable = true)\n",
      " |    |    |    |-- BLDG_NM: string (nullable = true)\n",
      " |    |    |    |-- BUILD_YEAR: string (nullable = true)\n",
      " |    |    |    |-- DEAL_YMD: string (nullable = true)\n",
      " |    |    |    |-- FLR_INFO: string (nullable = true)\n",
      " |    |    |    |-- JOB_GBN: string (nullable = true)\n",
      " |    |    |    |-- JOB_GBN_NM: string (nullable = true)\n",
      " |    |    |    |-- LAND_CD: string (nullable = true)\n",
      " |    |    |    |-- OBJ_AMT: string (nullable = true)\n",
      " |    |    |    |-- OBJ_SEQNO: string (nullable = true)\n",
      " |    |    |    |-- RIGHT_GBN: string (nullable = true)\n",
      " |    |    |    |-- RTMS_ID: string (nullable = true)\n",
      " |    |    |    |-- SGG_CD: string (nullable = true)\n",
      " |    |    |    |-- SGG_NM: string (nullable = true)\n",
      " |    |    |    |-- TOT_AREA: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "infoDf = spark.read.json(fname)\n",
    "infoDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|     landActualPriceInfo|\n",
      "+------------------------+\n",
      "|{{INFO-000, 정상 처리...|\n",
      "+------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "infoDf.select('landActualPriceInfo').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터프레임 조작"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 빈 데이터프레임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType([])\n",
    "emptyDf = spark.createDataFrame(spark.sparkContext.emptyRDD(), schema)\n",
    "emptyDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Range를 이용한 데이터프레임\n",
    "- 데이터프레임을 만들지 않고 함수를 실행하기에 적절하다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  2|\n",
      "|  4|\n",
      "|  6|\n",
      "|  8|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(0, 10, 2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|current_timestamp|\n",
      "+-----------------+\n",
      "|       1637540753|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "spark.range(1).select(F.unix_timestamp().alias('current_timestamp')).show()\n",
    "# 결과값은 rdd로 변환한 후 indexing을 통해 가져올 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 행 추가 및 삭제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### withColumn & drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- _c1: double (nullable = true)\n",
      " |-- _c2: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read\\\n",
    "    .options(header = 'false', inferschema = 'true', delimiter = '\\t')\\\n",
    "    .csv(os.path.join('data', 'spark_heightweight.txt'))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      " |-- weight: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('id', df['_c0'].cast('integer')).drop('_c0')\n",
    "df = df.withColumn('height', df['_c1'].cast('double')).drop('_c1')\n",
    "df = df.withColumn('weight', df['_c2'].cast('double')).drop('_c2')\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UDF : User Defined Functions\n",
    "- 함수명과 반환값, lambda함수를 가지고 만드는 사용자정의함수\n",
    "- withColumn등의 과정에서는 함수를 udf를 통해 호출해야 한다.\n",
    "- 코드가 복잡한 경우에 함수를 분리해서 처리할 수도 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      "\n",
      "+---+----+------+\n",
      "| id|name|height|\n",
      "+---+----+------+\n",
      "|  1| kim|  50.0|\n",
      "|  2| lee|  60.0|\n",
      "|  3|park|  70.0|\n",
      "+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.printSchema()\n",
    "myDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upper UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def uppercase(s):\n",
    "    return s.upper()\n",
    "\n",
    "upperUdf = udf(uppercase, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+---------+\n",
      "| id|name|height|nameUpper|\n",
      "+---+----+------+---------+\n",
      "|  1| kim|  50.0|      KIM|\n",
      "|  2| lee|  60.0|      LEE|\n",
      "|  3|park|  70.0|     PARK|\n",
      "+---+----+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.withColumn('nameUpper', upperUdf(myDf['name'])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 어디까지 줄일 수 있을까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+---------+\n",
      "| id|name|height|nameUpper|\n",
      "+---+----+------+---------+\n",
      "|  1| kim|  50.0|      KIM|\n",
      "|  2| lee|  60.0|      LEE|\n",
      "|  3|park|  70.0|     PARK|\n",
      "+---+----+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.withColumn('nameUpper', udf(lambda x : x.upper(),StringType())(myDf['name'])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분기형 idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+------------+\n",
      "| id|name|height|height > 175|\n",
      "+---+----+------+------------+\n",
      "|  1| kim|  50.0|     shorter|\n",
      "|  2| lee|  60.0|      taller|\n",
      "|  3|park|  70.0|      taller|\n",
      "+---+----+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "height_udf = udf(lambda height : 'taller' if height >= 60 else 'shorter', StringType())\n",
    "heightDf = myDf.withColumn('height > 175', height_udf(myDf['height']))\n",
    "heightDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 컬럼 명 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n",
      "| ID|height|weight|\n",
      "+---+------+------+\n",
      "|  1| 65.78|112.99|\n",
      "|  2| 71.52|136.49|\n",
      "|  3|  69.4|153.03|\n",
      "|  4| 68.22|142.34|\n",
      "|  5| 67.79| 144.3|\n",
      "+---+------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumnRenamed('id', 'ID').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### aggregate functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dictionary형식\n",
    "- agg() 함수에 dictionary형식으로 컬럼명: aggregate functions로 적어준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+\n",
      "|sum(weight)|      avg(height)|\n",
      "+-----------+-----------------+\n",
      "|     6442.1|68.05240000000002|\n",
      "+-----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg({'height' : 'avg', 'weight' : 'sum'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F함수\n",
    "- pyspark.sql.functions에 내장된 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|min(height)|\n",
      "+-----------+\n",
      "|      63.48|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "df.agg(F.min('height')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 조회 - select, where, groupby"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select\n",
    "- 부분 데이터프레임을 복사해 반환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|name| id|\n",
      "+----+---+\n",
      "| kim|  1|\n",
      "| lee|  2|\n",
      "|park|  3|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = ['name', 'id']\n",
    "withoutHeight = myDf.select('name', 'id') # *query 를 넣어주어도 작동함\n",
    "withoutHeight.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Column을 List로 변환하는 방법\n",
    "문제점: Row()로 구성된 컬럼에서 값만 빼기가 불편하다.  \n",
    "\n",
    "1. select한 결과를 rdd로 만든다.\n",
    "2. rdd를 x[0]으로 인덱싱하는 map을 거친다. (flatMap을 사용해도 된다.)\n",
    "3. collect해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kim', 'lee', 'park']"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDf.select('name').rdd.map(lambda x : x[0]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### select like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+---------------+\n",
      "|name|height|name LIKE %lee%|\n",
      "+----+------+---------------+\n",
      "| kim|  50.0|          false|\n",
      "| lee|  60.0|           true|\n",
      "|park|  70.0|          false|\n",
      "+----+------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.select('name', 'height', myDf['name'].like('%lee%')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### select startswith | endswith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-------------------+\n",
      "|name|height|startswith(name, k)|\n",
      "+----+------+-------------------+\n",
      "| kim|  50.0|               true|\n",
      "| lee|  60.0|              false|\n",
      "|park|  70.0|              false|\n",
      "+----+------+-------------------+\n",
      "\n",
      "+----+------+-----------------+\n",
      "|name|height|endswith(name, k)|\n",
      "+----+------+-----------------+\n",
      "| kim|  50.0|            false|\n",
      "| lee|  60.0|            false|\n",
      "|park|  70.0|             true|\n",
      "+----+------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.select('name', 'height', myDf.name.startswith('k')).show()\n",
    "myDf.select('name', 'height', myDf.name.endswith('k')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### alias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe의 이름도 바꿀 수 있다.\n",
    "myDf1 = myDf.alias('myDf1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|short name|\n",
      "+----------+\n",
      "|        ki|\n",
      "|        le|\n",
      "|        pa|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# columns의 이름을 바꾸는 데에 주로 쓰인다.\n",
    "myDf1.select(myDf1.name.substr(1, 2).alias('short name')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### when - otherwise : if - else문의 DF버전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------------------------------+\n",
      "|height|CASE WHEN (height < 60) THEN 1 ELSE 0 END|\n",
      "+------+-----------------------------------------+\n",
      "|  50.0|                                        1|\n",
      "|  60.0|                                        0|\n",
      "|  70.0|                                        0|\n",
      "+------+-----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "myDf.select('height', when(myDf.height < 60, 1).otherwise(0)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### where | filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+\n",
      "| id|name|height|\n",
      "+---+----+------+\n",
      "|  1| kim|  50.0|\n",
      "|  2| lee|  60.0|\n",
      "+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.where(myDf['height'] <= 60).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+\n",
      "| id|name|height|\n",
      "+---+----+------+\n",
      "|  1| kim|  50.0|\n",
      "|  2| lee|  60.0|\n",
      "+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.filter(myDf['height'] <= 60).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### regexp_replace : regular expression - 컬럼 내용 변경\n",
    "- regexp_replace(참조할 컬럼 명, 바뀌어질 대상 문자열, 바뀌는 결과 문자열)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+-------+\n",
      "| id|name|height|nameNew|\n",
      "+---+----+------+-------+\n",
      "|  1| kim|  50.0|    kim|\n",
      "|  2| lee|  60.0|    lim|\n",
      "|  3|park|  70.0|   park|\n",
      "+---+----+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "_heightDf = myDf.withColumn('nameNew', regexp_replace('name', 'lee', 'lim'))\n",
    "_heightDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### groupby\n",
    "- groupby나, groupBy나 같은 함수이다.\n",
    "- 특정 컬럼을 기준으로 구분지어서 각종 함수를 적용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-----------+\n",
      "|height|max(id)|max(height)|\n",
      "+------+-------+-----------+\n",
      "|  70.0|      3|       70.0|\n",
      "|  50.0|      1|       50.0|\n",
      "|  60.0|      2|       60.0|\n",
      "+------+-------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "myDf.groupby(myDf['height']).max().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### agg를 사용하는 groupby\n",
    "- avg, max, min, sum, count를 지원한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+\n",
      "|height|avg(height)|\n",
      "+------+-----------+\n",
      "|  70.0|       70.0|\n",
      "|  50.0|       50.0|\n",
      "|  60.0|       60.0|\n",
      "+------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "myDf.groupby('height').agg({'height' : 'avg'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pivot을 이용해 2차원 테이블 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----+----+\n",
      "|height| kim| lee|park|\n",
      "+------+----+----+----+\n",
      "|  70.0|null|null|   1|\n",
      "|  50.0|   1|null|null|\n",
      "|  60.0|null|   1|null|\n",
      "+------+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.groupBy('height').pivot('name').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 행 추가 : union()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+\n",
      "| id|name|height|\n",
      "+---+----+------+\n",
      "|  1| kim|  50.0|\n",
      "|  2| lee|  60.0|\n",
      "|  3|park|  70.0|\n",
      "|  4|keen|  78.0|\n",
      "+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "toAppendDf = spark.createDataFrame([Row(4, 'keen', 78.0)])\n",
    "myDf = myDf.union(toAppendDf)\n",
    "myDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition 조작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# partition 개수\n",
    "print(myDf.rdd.getNumPartitions())\n",
    "# partition 개수 조작\n",
    "_myDf = myDf.repartition(4) # 늘리거나 줄일 때\n",
    "# partition 개수 조작\n",
    "_myDf = _myDf.coalesce(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 통계 요약"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+----+------------------+\n",
      "|summary|                id|name|            height|\n",
      "+-------+------------------+----+------------------+\n",
      "|  count|                 4|   4|                 4|\n",
      "|   mean|               2.5|null|              64.5|\n",
      "| stddev|1.2909944487358056|null|12.151817422372122|\n",
      "|    min|                 1|keen|              50.0|\n",
      "|    max|                 4|park|              78.0|\n",
      "+-------+------------------+----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결측값"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df.na.fill()\n",
    "> 모든 컬럼의 na값을 0으로 교체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----+----+----+\n",
      "|height|keen| kim| lee|park|\n",
      "+------+----+----+----+----+\n",
      "|  70.0|null|null|null|   1|\n",
      "|  50.0|null|   1|null|null|\n",
      "|  78.0|   1|null|null|null|\n",
      "+------+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivotDf = myDf.groupBy('height').pivot('name').count()\n",
    "pivotDf.where(F.col('lee').isNull()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- gu: string (nullable = true)\n",
      " |-- area: double (nullable = true)\n",
      " |-- cost: long (nullable = true)\n",
      " |-- kind: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/22 12:49:40 WARN TaskSetManager: Stage 196 contains a task of very large size (1264 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "fname = os.path.join('data', 'LandActualPriceInfo.json')\n",
    "with open(fname, 'rb') as file:\n",
    "    data = file.readlines()\n",
    "data = json.loads(data[0])['landActualPriceInfo']['row']\n",
    "myRdd = spark.sparkContext.parallelize(data)\n",
    "myRowRdd = myRdd.map(lambda x: Row(gu = x['SGG_NM'], area = float(x['BLDG_AREA']), cost = int(x['OBJ_AMT']), kind = x['BLDG_MUSE_NM']))\n",
    "df = spark.createDataFrame(myRowRdd)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/22 12:51:57 WARN TaskSetManager: Stage 197 contains a task of very large size (1264 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+----+\n",
      "| gu|area|cost|kind|\n",
      "+---+----+----+----+\n",
      "|  0|   0|   0|   0|\n",
      "+---+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "cols = df.columns\n",
    "df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in cols]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sql string\n",
    "- sql을 문자열로 적으면 알아서 해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/22 14:57:46 WARN TaskSetManager: Stage 203 contains a task of very large size (1264 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---------+\n",
      "|    gu| area|     cost|\n",
      "+------+-----+---------+\n",
      "|송파구|23.88|340000000|\n",
      "+------+-----+---------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView('my')\n",
    "spark.sql('select gu, area, cost from my').show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### catalog로 view목록 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='my', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "구: 송파구\n",
      "구: 중구\n",
      "구: 동작구\n",
      "구: 성북구\n",
      "구: 송파구\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/22 15:43:41 WARN TaskSetManager: Stage 205 contains a task of very large size (1264 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "source": [
    "for e in df.rdd.map(lambda x: '구: ' + x[0]).take(5):\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### truncate\n",
    "- 출력할 때 긴 문자열을 잘라 출력할지 결정하는 인자이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucketDf = spark.createDataFrame([[1, ['orange', 'apple', 'pineapple']],\n",
    "                                   [2, ['watermelon', 'apple', 'bananas']]],\n",
    "                                 ['bucketId', 'items'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------------------+\n",
      "|bucketId|items                       |\n",
      "+--------+----------------------------+\n",
      "|1       |[orange, apple, pineapple]  |\n",
      "|2       |[watermelon, apple, bananas]|\n",
      "+--------+----------------------------+\n",
      "\n",
      "+--------+--------------------+\n",
      "|bucketId|               items|\n",
      "+--------+--------------------+\n",
      "|       1|[orange, apple, p...|\n",
      "|       2|[watermelon, appl...|\n",
      "+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bucketDf.show(3, truncate = False)\n",
    "bucketDf.show(3, truncate = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### explode\n",
    "- 컬럼에 List나 배열이 포함된 경우 flat해서 새로운 컬럼을 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "|bucketId|      item|\n",
      "+--------+----------+\n",
      "|       1|    orange|\n",
      "|       1|     apple|\n",
      "|       1| pineapple|\n",
      "|       2|watermelon|\n",
      "|       2|     apple|\n",
      "|       2|   bananas|\n",
      "+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "bDf = bucketDf.select(bucketDf.bucketId, explode(bucketDf.items).alias('item'))\n",
    "bDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### join\n",
    "- inner기준으로, item이 중복되는 것만 병합한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|      item|itemId|\n",
      "+----------+------+\n",
      "|    orange|    F1|\n",
      "|          |    F2|\n",
      "| pineapple|    F3|\n",
      "|watermelon|    F4|\n",
      "|   bananas|    F5|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fDf = spark.createDataFrame([['orange', 'F1'],\n",
    "                             ['', 'F2'],\n",
    "                             ['pineapple', 'F3'],\n",
    "                             ['watermelon', 'F4'],\n",
    "                             ['bananas', 'F5']], \n",
    "                            ['item', 'itemId'])\n",
    "fDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+--------+----------+\n",
      "|      item|itemId|bucketId|      item|\n",
      "+----------+------+--------+----------+\n",
      "|   bananas|    F5|       2|   bananas|\n",
      "|    orange|    F1|       1|    orange|\n",
      "| pineapple|    F3|       1| pineapple|\n",
      "|watermelon|    F4|       2|watermelon|\n",
      "+----------+------+--------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "joinDf = fDf.join(bDf, fDf.item == bDf.item, 'inner') # 합칠 대상, 조건, 속성\n",
    "joinDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 형변환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DateType 형변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1991-11-25 00:00:00\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "print(datetime.strptime('11/25/1991', '%m/%d/%Y'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "df = df.withColumn('date', to_date(df['myDate'], 'yyyy-MM-dd'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 범용적 해결책: cast()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---------+--------+-------+\n",
      "|    gu| area|     cost|    kind|areaInt|\n",
      "+------+-----+---------+--------+-------+\n",
      "|송파구|23.88|340000000|  아파트|     23|\n",
      "|  중구|46.74|420000000|오피스텔|     46|\n",
      "|동작구| 68.1|279000000|연립주택|     68|\n",
      "|성북구|29.96|330000000|연립주택|     29|\n",
      "|송파구|36.35|154000000|연립주택|     36|\n",
      "+------+-----+---------+--------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/22 14:54:28 WARN TaskSetManager: Stage 201 contains a task of very large size (1264 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "source": [
    "newDf = df.withColumn('areaInt', df['area'].cast(IntegerType()))\n",
    "newDf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---------+--------+-------+\n",
      "|    gu| area|     cost|    kind|areaInt|\n",
      "+------+-----+---------+--------+-------+\n",
      "|송파구|23.88|340000000|  아파트|     23|\n",
      "|  중구|46.74|420000000|오피스텔|     46|\n",
      "|동작구| 68.1|279000000|연립주택|     68|\n",
      "|성북구|29.96|330000000|연립주택|     29|\n",
      "|송파구|36.35|154000000|연립주택|     36|\n",
      "+------+-----+---------+--------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/22 14:55:06 WARN TaskSetManager: Stage 202 contains a task of very large size (1264 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "source": [
    "newDf = df.withColumn('areaInt', df['area'].cast('integer'))\n",
    "newDf.show(5)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0fe295ad4522a8f3a1c00f73e8c1fc4bffda693b9f244974c35f867e6985c135"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('Analysis': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
