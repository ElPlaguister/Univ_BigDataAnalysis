{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/22 03:40:21 WARN Utils: Your hostname, Kritiasui-MacBookAir.local resolves to a loopback address: 127.0.0.1; using 172.30.1.23 instead (on interface en0)\n",
      "21/11/22 03:40:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "21/11/22 03:40:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "myConf = pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession\\\n",
    "    .builder\\\n",
    "    .master('local')\\\n",
    "    .appName('myApp')\\\n",
    "    .config(conf = myConf)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List에서 생성\n",
    "- sparkContext.parallelize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "myList = [x for x in range(1, 8)]\n",
    "rdd1 = spark.sparkContext.parallelize(myList)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### file에서 생성\n",
    "- sparkContext.textFile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "rdd2 = spark.sparkContext.textFile(os.path.join('data', 'spark_wiki.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd3 = spark.sparkContext.textFile(os.path.join('data', 'spark_2cols.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API\n",
    "- 변환 함수를 사용해서 연산결과로서 새로운 RDD객체, iterator를 생성함\n",
    "- 연산 함수를 사용해서 연산결과로서 새로운 상수값을 python list로 생성함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transformations\n",
    "- Lazy연산으로, 실제 수행시에 일괄적으로 실행된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "[1, 4, 9, 16, 25, 36, 49]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map\n",
    "squared = rdd1.map(lambda x: x * x)\n",
    "squared.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['35', ' 2'], ['40', ' 27'], ['12', ' 38'], ['15', ' 31'], ['21', ' 1']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map\n",
    "splitted = rdd3.map(lambda line : line.split(','))\n",
    "splitted.take(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[35, 2], [40, 27], [12, 38], [15, 31], [21, 1]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map\n",
    "stripped = splitted.map(lambda x: [int(i) for i in x])\n",
    "stripped.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Wikipedia'],\n",
       " ['Apache',\n",
       "  'Spark',\n",
       "  'is',\n",
       "  'an',\n",
       "  'open',\n",
       "  'source',\n",
       "  'cluster',\n",
       "  'computing',\n",
       "  'framework.'],\n",
       " ['아파치', '스파크는', '오픈', '소스', '클러스터', '컴퓨팅', '프레임워크이다.']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = rdd2.map(lambda x : x.split())\n",
    "sentences.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia \n",
      "-----\n",
      "Apache Spark is an open source cluster computing framework. \n",
      "-----\n",
      "아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다. \n",
      "-----\n",
      "Apache Spark Apache Spark Apache Spark \n",
      "-----\n",
      "아파치 스파크 아파치 스파크 아파치 스파크 \n",
      "-----\n",
      "Originally developed at the University of California, Berkeley's AMPLab, \n",
      "-----\n",
      "the Spark codebase was later donated to the Apache Software Foundation, \n",
      "-----\n",
      "which has maintained it since. \n",
      "-----\n",
      "Spark provides an interface for programming entire clusters with \n",
      "-----\n",
      "implicit data parallelism and fault-tolerance. \n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "for line in sentences.collect():\n",
    "    for word in line:\n",
    "        print(word, end = ' ')\n",
    "    print(\"\\n-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 59, 32, 38, 23, 72, 71, 30, 64, 46]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.map(lambda x : len(x)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5050"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_ztoh = spark.sparkContext.parallelize(range(1, 101))\n",
    "rdd_ztoh.reduce(lambda subtotal, x: subtotal + x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many lines having 'Spark': 4\n"
     ]
    }
   ],
   "source": [
    "# 한글 필터링은 문자열 앞에 u붙여서 unicode인식하게 해주기\n",
    "rdd_filtered = rdd2.filter(lambda line : 'Spark' in line)\n",
    "print(\"How many lines having 'Spark':\", rdd_filtered.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia Apache Spark open source cluster computing framework. 아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다. Apache Spark Apache Spark Apache Spark 아파치 스파크 아파치 스파크 아파치 스파크 Originally developed University of California, Berkeley's AMPLab, Spark codebase was later donated to Apache Software Foundation, which has maintained it since. Spark provides interface programming entire clusters with implicit data parallelism and fault-tolerance. "
     ]
    }
   ],
   "source": [
    "# filter로 stopwords제거\n",
    "stopwords = ['is', 'am', 'are', 'the', 'for', 'a', 'an', 'at']\n",
    "stop_filtered = rdd2.flatMap(lambda x : x.split()).filter(lambda x: x not in stopwords).collect()\n",
    "for words in stop_filtered:\n",
    "    print(words, end = ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### foreach\n",
    "-   반환값이 없고, 각 요소에 대해 적용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'collect'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/lr/jj1kxjwn315g3cjyb46358l80000gn/T/ipykernel_24105/2711268678.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforeach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'collect'"
     ]
    }
   ],
   "source": [
    "spark.sparkContext.parallelize(range(1, 6)).foreach(lambda x : x + 1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1 2 3 4 5 "
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    print(x, end = ' ')\n",
    "\n",
    "spark.sparkContext.parallelize(range(1, 6)).foreach(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### groupBy\n",
    "- 주로 unpaired RDD에 많이 쓰인다.\n",
    "- key를 선택하여 사용할 수 있다.\n",
    "- 유사한 기능을 수행하는 groupByKey()와 비교해서 상대적으로 빠르지 않다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elplaguister/opt/anaconda3/envs/Analysis/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py:60: UserWarning: Please install psutil to have better support with spilling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wi : <pyspark.resultiterable.ResultIterable object at 0x7f9bc7355a30>\n",
      "Ap : <pyspark.resultiterable.ResultIterable object at 0x7f9bc7355f10>\n",
      "아파 : <pyspark.resultiterable.ResultIterable object at 0x7f9bc7355640>\n",
      "Or : <pyspark.resultiterable.ResultIterable object at 0x7f9bc750cd90>\n",
      "th : <pyspark.resultiterable.ResultIterable object at 0x7f9bc750cca0>\n",
      "wh : <pyspark.resultiterable.ResultIterable object at 0x7f9bc750c220>\n",
      "Sp : <pyspark.resultiterable.ResultIterable object at 0x7f9bc750c2e0>\n",
      "im : <pyspark.resultiterable.ResultIterable object at 0x7f9bc750c250>\n"
     ]
    }
   ],
   "source": [
    "# 앞 2글자로 group\n",
    "grouping = rdd2.groupBy(lambda x : x[:2]) # 인자로 key를 지정해준다.\n",
    "for (k, v) in grouping.collect():\n",
    "    print(k, ':', v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wi: Wikipedia\n",
      "----\n",
      "Ap: Apache Spark is an open source cluster computing framework.\n",
      "Ap: Apache Spark Apache Spark Apache Spark\n",
      "----\n",
      "아파: 아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.\n",
      "아파: 아파치 스파크 아파치 스파크 아파치 스파크\n",
      "----\n",
      "Or: Originally developed at the University of California, Berkeley's AMPLab,\n",
      "----\n",
      "th: the Spark codebase was later donated to the Apache Software Foundation,\n",
      "----\n",
      "wh: which has maintained it since.\n",
      "----\n",
      "Sp: Spark provides an interface for programming entire clusters with\n",
      "----\n",
      "im: implicit data parallelism and fault-tolerance.\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "grouping = rdd2.groupBy(lambda x: x[:2])\n",
    "for k, v in grouping.collect():\n",
    "    for eachValue in v:\n",
    "        print(\"{}: {}\".format(k, eachValue))\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### paired RDD의 groupBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "_testList = [('a', 1), ('a', 1), ('a', 1), ('b', 1), ('b', 1),\n",
    "             ('a', 1), ('b', 1),\n",
    "             ('a', 1), ('a', 1), ('b', 1), ('b', 1),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', <pyspark.resultiterable.ResultIterable at 0x7f9bc75081f0>),\n",
       " ('b', <pyspark.resultiterable.ResultIterable at 0x7f9bc754c430>)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_testRdd = spark.sparkContext.parallelize(_testList)\n",
    "_testRdd.groupBy(lambda x: x[0]).collect()\n",
    "# groupBy의 결과는 (key, resultIteratable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### resultIteratable을 보는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', [('a', 1), ('a', 1), ('a', 1), ('a', 1), ('a', 1), ('a', 1)]),\n",
       " ('b', [('b', 1), ('b', 1), ('b', 1), ('b', 1), ('b', 1)])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_testRdd.groupBy(lambda x : x[0]).mapValues(list).collect()\n",
    "# mapValues : value에 대해 map연산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pair RDD에 대한 연산\n",
    "- groupByKey는 메모리를 많이 사용해서 비추\n",
    "- reduceByKey나 combineByKey를 사용해서 병렬처리를 지향하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd4 = _testRdd\n",
    "rdd4.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'a', 'a', 'b', 'b', 'a', 'b', 'a', 'a', 'b', 'b']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd4.keys().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reduceByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 6), ('b', 5)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd4.reduceByKey(lambda x, y: x + y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Wikipedia', 1), ('Apache', 5), ('Spark', 6)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# groupByKey를 사용했을 때의 예\n",
    "rdd2\\\n",
    "    .flatMap(lambda x: x.split())\\\n",
    "    .map(lambda x: (x, 1))\\\n",
    "    .groupByKey()\\\n",
    "    .mapValues(sum)\\\n",
    "    .take(3)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Wikipedia', 1), ('Apache', 5), ('Spark', 6)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduceByKey를 사용했을 때의 예\n",
    "rdd2\\\n",
    "    .flatMap(lambda x: x.split())\\\n",
    "    .map(lambda x: (x, 1))\\\n",
    "    .reduceByKey(lambda x, y : x + y)\\\n",
    "    .take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('Wikipedia', 1), ('Apache', 5), ('Spark', 6), ('is', 1), ('an', 2), ('open', 1), ('source', 1), ('cluster', 1), ('computing', 1), ('framework.', 1), ('아파치', 4), ('스파크는', 1), ('오픈', 1), ('소스', 1), ('클러스터', 1), ('컴퓨팅', 1), ('프레임워크이다.', 1), ('스파크', 3), ('Originally', 1), ('developed', 1), ('at', 1), ('the', 3), ('University', 1), ('of', 1), ('California,', 1), (\"Berkeley's\", 1), ('AMPLab,', 1), ('codebase', 1), ('was', 1), ('later', 1), ('donated', 1), ('to', 1), ('Software', 1), ('Foundation,', 1), ('which', 1), ('has', 1), ('maintained', 1), ('it', 1), ('since.', 1), ('provides', 1), ('interface', 1), ('for', 1), ('programming', 1), ('entire', 1), ('clusters', 1), ('with', 1), ('implicit', 1), ('data', 1), ('parallelism', 1), ('and', 1), ('fault-tolerance.', 1)])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#countByKey를 사용하는 경우의 예\n",
    "rdd2\\\n",
    "    .flatMap(lambda x : x.split())\\\n",
    "    .map(lambda x : (x, 1))\\\n",
    "    .countByKey()\\\n",
    "    .items()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### combineByKey\n",
    "rdd.combineByKey(createCombiner, mergeValue, mergeCombiner)\n",
    "- createCombiner : 각 값을 연산에 필요한 형태로 바꾼다.\n",
    "- mergeValue : accumulator, new value를 인자로 받아, 각 partition별로 연산을 수행한다.\n",
    "- mergeCombiner : accumulator, accumulator를 인자로 받아, partition간의 연산을 수행한다.\n",
    "\n",
    "1. reduceByKey를 사용할 수 없을 때 사용합니다.\n",
    "2. 평균과 합계를 구할 때에도 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', (6, 6)), ('b', (5, 5))]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/22 20:57:33 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 1531100 ms exceeds timeout 120000 ms\n",
      "21/11/22 20:57:33 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "rdd4.combineByKey(lambda value : (value, 1),\n",
    "                  lambda x, value : (x[0] + value, x[1] + 1),\n",
    "                  lambda x, y : (x[0] + y[0], x[1] + y[1]))\\\n",
    "    .collect()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0fe295ad4522a8f3a1c00f73e8c1fc4bffda693b9f244974c35f867e6985c135"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('Analysis': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
