{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9afc5d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21cad134",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/26 05:00:32 WARN Utils: Your hostname, Kritiasui-MacBookAir.local resolves to a loopback address: 127.0.0.1; using 172.30.1.27 instead (on interface en0)\n",
      "21/09/26 05:00:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "21/09/26 05:00:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "myConf = pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession.builder.master('local').appName('myApp').config(conf=myConf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f51d7dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "�������,�α���(��),�α���(��),�α���(��),������(��),������(��),������(��),����,�����,������α�,���������,�����μ���,�μ���ȭ��ȣ,�����ͱ�������\n",
      "������1��,32292,16538,15754,6.97,3.57,3.4,104.98,19998,1.61,�����ν�,�ο����ǰ�,031-828-2466,2021-09-10\n",
      "������2��,31380,15608,15772,6.77,3.37,3.4,98.96,16410,1.91,�����ν�,�ο����ǰ�,031-828-2466,2021-09-10\n",
      "ȣ��1��,36124,17595,18529,7.8,3.8,4,94.96,15653,2.31,�����ν�,�ο����ǰ�,031-828-2466,2021-09-10\n",
      "ȣ��2��,34957,16923,18034,7.54,3.65,3.89,93.84,13683,2.55,�����ν�,�ο����ǰ�,031-828-2466,2021-09-10\n"
     ]
    }
   ],
   "source": [
    "popRdd = spark.sparkContext.textFile(os.path.join('data', 'first.csv'), use_unicode = True)\n",
    "for i in popRdd.take(5):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3e3002d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "������,�������� �α���,65���̻� �α��� ,14������ �α���,���ȭ����,���ȭ����,�����ͱ�������\n",
      "2012,154057,25826,22861,16.76,112.97,2021-08-31\n",
      "2013,155641,26936,22393,17.31,120.29,2021-08-31\n",
      "2014,158512,27877,22058,17.59,126.38,2021-08-31\n",
      "2015,164519,28979,22362,17.61,129.59,2021-08-31\n"
     ]
    }
   ],
   "source": [
    "agedRdd = spark.sparkContext.textFile(os.path.join('data', 'second.csv'), use_unicode = True)\n",
    "for i in agedRdd.take(5):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4e48102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['행정기관,인구수(계),인구수(남),인구수(여),구성비(계),구성비(남),구성비(여),성비,세대수,세대당인구,관리기관명,관리부서명,부서전화번호,데이터기준일자\\r\\n의정부1동,32292,16538,15754,6.97,3.57,3.4,104.98,19998,1.61,의정부시,민원여권과,031-828-2466,2021-09-10\\r\\n의정부2동,31380,15608,15772,6.77,3.37,3.4,98.96,16410,1.91,의정부시,민원여권과,031-828-2466,2021-09-10\\r\\n호원1동,36124,17595,18529,7.8,3.8,4,94.96,15653,2.31,의정부시,민원여권과,031-828-2466,2021-09-10\\r\\n호원2동,34957,16923,18034,7.54,3.65,3.89,93.84,13683,2.55,의정부시,민원여권과,031-828-2466,2021-09-10\\r\\n장암동,20314,9714,10600,4.38,2.1,2.29,91.64,8604,2.36,의정부시,민원여권과,031-828-2466,2021-09-10\\r\\n신곡1동,43159,21205,21954,9.31,4.58,4.74,96.59,17990,2.4,의정부시,민원여권과,031-828-2466,2021-09-10\\r\\n신곡2동,47852,23232,24620,10.33,5.01,5.31,94.36,19218,2.49,의정부시,민원여권과,031-828-2466,2021-09-10\\r\\n송산1동,42817,21276,21541,9.24,4.59,4.65,98.77,18811,2.28,의정부시,민원여권과,031-828-2466,2021-09-10\\r\\n송산2동,33565,16601,16964,7.24,3.58,3.66,97.86,13216,2.54,의정부시,민원여권과,031-828-2466,2021-09-10\\r\\n송산3동,46892,22772,24120,10.12,4.91,5.21,94.41,17926,2.62,의정부시,민원여권과,031-828-2466,2021-09-10\\r\\n자금동,27087,13270,13817,5.85,2.86,2.98,96.04,11868,2.28,의정부시,민원여권과,031-828-2466,2021-09-10\\r\\n가능동,25990,12974,13016,5.61,2.8,2.81,99.68,12492,2.08,의정부시,민원여권과,031-828-2466,2021-09-10\\r\\n흥선동,19176,9769,9407,4.14,2.11,2.03,103.85,9380,2.04,의정부시,민원여권과,031-828-2466,2021-09-10\\r\\n녹양동,21768,10872,10896,4.7,2.35,2.35,99.78,9556,2.28,의정부시,민원여권과,031-828-2466,2021-09-10\\r\\n']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "popRddBin = spark.sparkContext.binaryFiles(os.path.join('data', 'first.csv'))\n",
    "_my = popRddBin.map(lambda x : x[1].decode('euc-kr'))\n",
    "_my.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a3fabd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'행정기관,인구수(계),인구수(남),인구수(여),구성비(계),구성비(남),구성비(여),성비,세대수,세대당인구,관리기관명,관리부서명,부서전화번호,데이터기준일자'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "popList = _my.map(lambda x : x.split()).take(3)\n",
    "popList[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5e29bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "popDf = spark\\\n",
    ".read.option('charset', 'euc-kr')\\\n",
    ".option('header', 'true')\\\n",
    ".csv(os.path.join('data', 'first.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71c766b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+----------+----------+----------+----------+----------+------+------+----------+----------+----------+------------+--------------+\n",
      "| 행정기관|인구수(계)|인구수(남)|인구수(여)|구성비(계)|구성비(남)|구성비(여)|  성비|세대수|세대당인구|관리기관명|관리부서명|부서전화번호|데이터기준일자|\n",
      "+---------+----------+----------+----------+----------+----------+----------+------+------+----------+----------+----------+------------+--------------+\n",
      "|의정부1동|     32292|     16538|     15754|      6.97|      3.57|       3.4|104.98| 19998|      1.61|  의정부시|민원여권과|031-828-2466|    2021-09-10|\n",
      "|의정부2동|     31380|     15608|     15772|      6.77|      3.37|       3.4| 98.96| 16410|      1.91|  의정부시|민원여권과|031-828-2466|    2021-09-10|\n",
      "|  호원1동|     36124|     17595|     18529|       7.8|       3.8|         4| 94.96| 15653|      2.31|  의정부시|민원여권과|031-828-2466|    2021-09-10|\n",
      "|  호원2동|     34957|     16923|     18034|      7.54|      3.65|      3.89| 93.84| 13683|      2.55|  의정부시|민원여권과|031-828-2466|    2021-09-10|\n",
      "|   장암동|     20314|      9714|     10600|      4.38|       2.1|      2.29| 91.64|  8604|      2.36|  의정부시|민원여권과|031-828-2466|    2021-09-10|\n",
      "+---------+----------+----------+----------+----------+----------+----------+------+------+----------+----------+----------+------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "popDf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7331c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "agedDf = spark\\\n",
    ".read.option('charset', 'euc-kr')\\\n",
    ".option('header', 'true')\\\n",
    ".csv(os.path.join('data', 'second.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "935a3642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+----------------+---------------+----------+----------+--------------+\n",
      "|연도별|서귀포시 인구수|65세이상 인구수 |14세이하 인구수|고령화비율|노령화지수|데이터기준일자|\n",
      "+------+---------------+----------------+---------------+----------+----------+--------------+\n",
      "|  2012|         154057|           25826|          22861|     16.76|    112.97|    2021-08-31|\n",
      "|  2013|         155641|           26936|          22393|     17.31|    120.29|    2021-08-31|\n",
      "|  2014|         158512|           27877|          22058|     17.59|    126.38|    2021-08-31|\n",
      "|  2015|         164519|           28979|          22362|     17.61|    129.59|    2021-08-31|\n",
      "|  2016|         170932|           30030|          23044|     17.57|    130.32|    2021-08-31|\n",
      "+------+---------------+----------------+---------------+----------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agedDf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ff71e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/ds3_popCsvRead.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/ds3_popCsvRead.py\n",
    "import os\n",
    "import pyspark\n",
    "\n",
    "def doIt():\n",
    "    print('----------Result----------')\n",
    "    popDf = spark\\\n",
    "    .read.option('charset', 'euc-kr')\\\n",
    "    .option('header', 'true')\\\n",
    "    .csv(os.path.join('data', 'first.csv'))\n",
    "\n",
    "    popDf.show(5)\n",
    "    \n",
    "    agedDf = spark\\\n",
    "    .read.option('charset', 'euc-kr')\\\n",
    "    .option('header', 'true')\\\n",
    "    .csv(os.path.join('data', 'second.csv'))\n",
    "    \n",
    "    agedDf.show(5)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    myConf = pyspark.SparkConf()\n",
    "    spark = pyspark.sql.SparkSession.builder.master('local').appName('myApp').config(conf=myConf).getOrCreate()\n",
    "    doIt()\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b71f6335",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/09/26 05:40:40 WARN Utils: Your hostname, Kritiasui-MacBookAir.local resolves to a loopback address: 127.0.0.1; using 172.30.1.27 instead (on interface en0)\n",
      "21/09/26 05:40:40 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "21/09/26 05:40:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "21/09/26 05:40:41 INFO SparkContext: Running Spark version 3.1.2\n",
      "21/09/26 05:40:41 INFO ResourceUtils: ==============================================================\n",
      "21/09/26 05:40:41 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "21/09/26 05:40:41 INFO ResourceUtils: ==============================================================\n",
      "21/09/26 05:40:41 INFO SparkContext: Submitted application: myApp\n",
      "21/09/26 05:40:42 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "21/09/26 05:40:42 INFO ResourceProfile: Limiting resource is cpu\n",
      "21/09/26 05:40:42 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "21/09/26 05:40:42 INFO SecurityManager: Changing view acls to: elplaguister\n",
      "21/09/26 05:40:42 INFO SecurityManager: Changing modify acls to: elplaguister\n",
      "21/09/26 05:40:42 INFO SecurityManager: Changing view acls groups to: \n",
      "21/09/26 05:40:42 INFO SecurityManager: Changing modify acls groups to: \n",
      "21/09/26 05:40:42 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(elplaguister); groups with view permissions: Set(); users  with modify permissions: Set(elplaguister); groups with modify permissions: Set()\n",
      "21/09/26 05:40:42 INFO Utils: Successfully started service 'sparkDriver' on port 58346.\n",
      "21/09/26 05:40:42 INFO SparkEnv: Registering MapOutputTracker\n",
      "21/09/26 05:40:42 INFO SparkEnv: Registering BlockManagerMaster\n",
      "21/09/26 05:40:42 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "21/09/26 05:40:42 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "21/09/26 05:40:42 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "21/09/26 05:40:42 INFO DiskBlockManager: Created local directory at /private/var/folders/lr/jj1kxjwn315g3cjyb46358l80000gn/T/blockmgr-5ff64761-2f28-47ab-8267-3db0ddb34321\n",
      "21/09/26 05:40:42 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
      "21/09/26 05:40:42 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "21/09/26 05:40:43 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "21/09/26 05:40:43 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
      "21/09/26 05:40:43 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://172.30.1.27:4041\n",
      "21/09/26 05:40:43 INFO Executor: Starting executor ID driver on host 172.30.1.27\n",
      "21/09/26 05:40:43 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 58347.\n",
      "21/09/26 05:40:43 INFO NettyBlockTransferService: Server created on 172.30.1.27:58347\n",
      "21/09/26 05:40:43 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "21/09/26 05:40:43 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 172.30.1.27, 58347, None)\n",
      "21/09/26 05:40:43 INFO BlockManagerMasterEndpoint: Registering block manager 172.30.1.27:58347 with 366.3 MiB RAM, BlockManagerId(driver, 172.30.1.27, 58347, None)\n",
      "21/09/26 05:40:43 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 172.30.1.27, 58347, None)\n",
      "21/09/26 05:40:43 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 172.30.1.27, 58347, None)\n",
      "21/09/26 05:40:44 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/elplaguister/Workspace/Univ_BigDataAnalysis/Week4/spark-warehouse').\n",
      "21/09/26 05:40:44 INFO SharedState: Warehouse path is 'file:/Users/elplaguister/Workspace/Univ_BigDataAnalysis/Week4/spark-warehouse'.\n",
      "----------Result----------\n",
      "21/09/26 05:40:46 INFO InMemoryFileIndex: It took 75 ms to list leaf files for 1 paths.\n",
      "21/09/26 05:40:46 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.\n",
      "21/09/26 05:40:48 INFO FileSourceStrategy: Pushed Filters: \n",
      "21/09/26 05:40:48 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "21/09/26 05:40:48 INFO FileSourceStrategy: Output Data Schema: struct<value: string>\n",
      "21/09/26 05:40:49 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.5 KiB, free 366.0 MiB)\n",
      "21/09/26 05:40:49 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 366.0 MiB)\n",
      "21/09/26 05:40:49 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.30.1.27:58347 (size: 27.5 KiB, free: 366.3 MiB)\n",
      "21/09/26 05:40:49 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0\n",
      "21/09/26 05:40:49 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4195881 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "21/09/26 05:40:50 INFO CodeGenerator: Code generated in 269.707367 ms\n",
      "21/09/26 05:40:50 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
      "21/09/26 05:40:50 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "21/09/26 05:40:50 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)\n",
      "21/09/26 05:40:50 INFO DAGScheduler: Parents of final stage: List()\n",
      "21/09/26 05:40:50 INFO DAGScheduler: Missing parents: List()\n",
      "21/09/26 05:40:50 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[6] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "21/09/26 05:40:50 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.3 KiB, free 366.0 MiB)\n",
      "21/09/26 05:40:50 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.7 KiB, free 366.0 MiB)\n",
      "21/09/26 05:40:50 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.30.1.27:58347 (size: 6.7 KiB, free: 366.3 MiB)\n",
      "21/09/26 05:40:50 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1388\n",
      "21/09/26 05:40:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[6] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "21/09/26 05:40:50 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "21/09/26 05:40:50 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.30.1.27, executor driver, partition 0, PROCESS_LOCAL, 4903 bytes) taskResourceAssignments Map()\n",
      "21/09/26 05:40:50 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "21/09/26 05:40:51 INFO FileScanRDD: Reading File path: file:///Users/elplaguister/Workspace/Univ_BigDataAnalysis/Week4/data/first.csv, range: 0-1577, partition values: [empty row]\n",
      "21/09/26 05:40:51 INFO CodeGenerator: Code generated in 24.175215 ms\n",
      "21/09/26 05:40:51 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1770 bytes result sent to driver\n",
      "21/09/26 05:40:51 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 852 ms on 172.30.1.27 (executor driver) (1/1)\n",
      "21/09/26 05:40:51 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "21/09/26 05:40:51 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 1.130 s\n",
      "21/09/26 05:40:51 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "21/09/26 05:40:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "21/09/26 05:40:51 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 1.220378 s\n",
      "21/09/26 05:40:51 INFO CodeGenerator: Code generated in 41.393809 ms\n",
      "21/09/26 05:40:52 INFO FileSourceStrategy: Pushed Filters: \n",
      "21/09/26 05:40:52 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "21/09/26 05:40:52 INFO FileSourceStrategy: Output Data Schema: struct<행정기관: string, 인구수(계): string, 인구수(남): string, 인구수(여): string, 구성비(계): string ... 12 more fields>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/09/26 05:40:52 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.3 KiB, free 365.7 MiB)\n",
      "21/09/26 05:40:52 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.6 MiB)\n",
      "21/09/26 05:40:52 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.30.1.27:58347 (size: 27.5 KiB, free: 366.2 MiB)\n",
      "21/09/26 05:40:52 INFO SparkContext: Created broadcast 2 from showString at NativeMethodAccessorImpl.java:0\n",
      "21/09/26 05:40:52 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4195881 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "21/09/26 05:40:52 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "21/09/26 05:40:52 INFO DAGScheduler: Got job 1 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "21/09/26 05:40:52 INFO DAGScheduler: Final stage: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0)\n",
      "21/09/26 05:40:52 INFO DAGScheduler: Parents of final stage: List()\n",
      "21/09/26 05:40:52 INFO DAGScheduler: Missing parents: List()\n",
      "21/09/26 05:40:52 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[13] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "21/09/26 05:40:52 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 10.4 KiB, free 365.6 MiB)\n",
      "21/09/26 05:40:52 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 365.6 MiB)\n",
      "21/09/26 05:40:52 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.30.1.27:58347 (size: 5.5 KiB, free: 366.2 MiB)\n",
      "21/09/26 05:40:52 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1388\n",
      "21/09/26 05:40:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[13] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "21/09/26 05:40:52 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
      "21/09/26 05:40:52 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.30.1.27, executor driver, partition 0, PROCESS_LOCAL, 4903 bytes) taskResourceAssignments Map()\n",
      "21/09/26 05:40:52 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
      "21/09/26 05:40:52 INFO FileScanRDD: Reading File path: file:///Users/elplaguister/Workspace/Univ_BigDataAnalysis/Week4/data/first.csv, range: 0-1577, partition values: [empty row]\n",
      "21/09/26 05:40:52 INFO CodeGenerator: Code generated in 34.154168 ms\n",
      "21/09/26 05:40:52 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2046 bytes result sent to driver\n",
      "21/09/26 05:40:52 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 123 ms on 172.30.1.27 (executor driver) (1/1)\n",
      "21/09/26 05:40:52 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "21/09/26 05:40:52 INFO DAGScheduler: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 0.142 s\n",
      "21/09/26 05:40:52 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "21/09/26 05:40:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "21/09/26 05:40:52 INFO DAGScheduler: Job 1 finished: showString at NativeMethodAccessorImpl.java:0, took 0.150895 s\n",
      "21/09/26 05:40:52 INFO CodeGenerator: Code generated in 56.701892 ms\n",
      "+---------+----------+----------+----------+----------+----------+----------+------+------+----------+----------+----------+------------+--------------+\n",
      "| 행정기관|인구수(계)|인구수(남)|인구수(여)|구성비(계)|구성비(남)|구성비(여)|  성비|세대수|세대당인구|관리기관명|관리부서명|부서전화번호|데이터기준일자|\n",
      "+---------+----------+----------+----------+----------+----------+----------+------+------+----------+----------+----------+------------+--------------+\n",
      "|의정부1동|     32292|     16538|     15754|      6.97|      3.57|       3.4|104.98| 19998|      1.61|  의정부시|민원여권과|031-828-2466|    2021-09-10|\n",
      "|의정부2동|     31380|     15608|     15772|      6.77|      3.37|       3.4| 98.96| 16410|      1.91|  의정부시|민원여권과|031-828-2466|    2021-09-10|\n",
      "|  호원1동|     36124|     17595|     18529|       7.8|       3.8|         4| 94.96| 15653|      2.31|  의정부시|민원여권과|031-828-2466|    2021-09-10|\n",
      "|  호원2동|     34957|     16923|     18034|      7.54|      3.65|      3.89| 93.84| 13683|      2.55|  의정부시|민원여권과|031-828-2466|    2021-09-10|\n",
      "|   장암동|     20314|      9714|     10600|      4.38|       2.1|      2.29| 91.64|  8604|      2.36|  의정부시|민원여권과|031-828-2466|    2021-09-10|\n",
      "+---------+----------+----------+----------+----------+----------+----------+------+------+----------+----------+----------+------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "21/09/26 05:40:52 INFO InMemoryFileIndex: It took 4 ms to list leaf files for 1 paths.\n",
      "21/09/26 05:40:52 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.\n",
      "21/09/26 05:40:52 INFO FileSourceStrategy: Pushed Filters: \n",
      "21/09/26 05:40:52 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "21/09/26 05:40:52 INFO FileSourceStrategy: Output Data Schema: struct<value: string>\n",
      "21/09/26 05:40:52 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 303.5 KiB, free 365.3 MiB)\n",
      "21/09/26 05:40:52 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.3 MiB)\n",
      "21/09/26 05:40:52 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.30.1.27:58347 (size: 27.5 KiB, free: 366.2 MiB)\n",
      "21/09/26 05:40:52 INFO SparkContext: Created broadcast 4 from csv at NativeMethodAccessorImpl.java:0\n",
      "21/09/26 05:40:52 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194887 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "21/09/26 05:40:52 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
      "21/09/26 05:40:52 INFO DAGScheduler: Got job 2 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "21/09/26 05:40:52 INFO DAGScheduler: Final stage: ResultStage 2 (csv at NativeMethodAccessorImpl.java:0)\n",
      "21/09/26 05:40:52 INFO DAGScheduler: Parents of final stage: List()\n",
      "21/09/26 05:40:52 INFO DAGScheduler: Missing parents: List()\n",
      "21/09/26 05:40:52 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[20] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "21/09/26 05:40:52 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 14.3 KiB, free 365.3 MiB)\n",
      "21/09/26 05:40:52 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 6.7 KiB, free 365.3 MiB)\n",
      "21/09/26 05:40:52 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.30.1.27:58347 (size: 6.7 KiB, free: 366.2 MiB)\n",
      "21/09/26 05:40:52 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1388\n",
      "21/09/26 05:40:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[20] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "21/09/26 05:40:52 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
      "21/09/26 05:40:52 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (172.30.1.27, executor driver, partition 0, PROCESS_LOCAL, 4904 bytes) taskResourceAssignments Map()\n",
      "21/09/26 05:40:52 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
      "21/09/26 05:40:52 INFO FileScanRDD: Reading File path: file:///Users/elplaguister/Workspace/Univ_BigDataAnalysis/Week4/data/second.csv, range: 0-583, partition values: [empty row]\n",
      "21/09/26 05:40:52 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1695 bytes result sent to driver\n",
      "21/09/26 05:40:52 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 26 ms on 172.30.1.27 (executor driver) (1/1)\n",
      "21/09/26 05:40:52 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "21/09/26 05:40:52 INFO DAGScheduler: ResultStage 2 (csv at NativeMethodAccessorImpl.java:0) finished in 0.047 s\n",
      "21/09/26 05:40:52 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "21/09/26 05:40:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
      "21/09/26 05:40:52 INFO DAGScheduler: Job 2 finished: csv at NativeMethodAccessorImpl.java:0, took 0.054466 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/09/26 05:40:53 INFO FileSourceStrategy: Pushed Filters: \n",
      "21/09/26 05:40:53 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "21/09/26 05:40:53 INFO FileSourceStrategy: Output Data Schema: struct<연도별: string, 서귀포시 인구수: string, 65세이상 인구수 : string, 14세이하 인구수: string, 고령화비율: string ... 5 more fields>\n",
      "21/09/26 05:40:53 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 303.3 KiB, free 365.0 MiB)\n",
      "21/09/26 05:40:53 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 27.5 KiB, free 365.0 MiB)\n",
      "21/09/26 05:40:53 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.30.1.27:58347 (size: 27.5 KiB, free: 366.2 MiB)\n",
      "21/09/26 05:40:53 INFO SparkContext: Created broadcast 6 from showString at NativeMethodAccessorImpl.java:0\n",
      "21/09/26 05:40:53 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194887 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "21/09/26 05:40:53 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "21/09/26 05:40:53 INFO DAGScheduler: Got job 3 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "21/09/26 05:40:53 INFO DAGScheduler: Final stage: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0)\n",
      "21/09/26 05:40:53 INFO DAGScheduler: Parents of final stage: List()\n",
      "21/09/26 05:40:53 INFO DAGScheduler: Missing parents: List()\n",
      "21/09/26 05:40:53 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[27] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "21/09/26 05:40:53 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 9.7 KiB, free 364.9 MiB)\n",
      "21/09/26 05:40:53 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 5.3 KiB, free 364.9 MiB)\n",
      "21/09/26 05:40:53 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.30.1.27:58347 (size: 5.3 KiB, free: 366.2 MiB)\n",
      "21/09/26 05:40:53 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1388\n",
      "21/09/26 05:40:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[27] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "21/09/26 05:40:53 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
      "21/09/26 05:40:53 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (172.30.1.27, executor driver, partition 0, PROCESS_LOCAL, 4904 bytes) taskResourceAssignments Map()\n",
      "21/09/26 05:40:53 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)\n",
      "21/09/26 05:40:53 INFO FileScanRDD: Reading File path: file:///Users/elplaguister/Workspace/Univ_BigDataAnalysis/Week4/data/second.csv, range: 0-583, partition values: [empty row]\n",
      "21/09/26 05:40:53 INFO CodeGenerator: Code generated in 31.527355 ms\n",
      "21/09/26 05:40:53 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1690 bytes result sent to driver\n",
      "21/09/26 05:40:53 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 64 ms on 172.30.1.27 (executor driver) (1/1)\n",
      "21/09/26 05:40:53 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "21/09/26 05:40:53 INFO DAGScheduler: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0) finished in 0.082 s\n",
      "21/09/26 05:40:53 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "21/09/26 05:40:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
      "21/09/26 05:40:53 INFO DAGScheduler: Job 3 finished: showString at NativeMethodAccessorImpl.java:0, took 0.092800 s\n",
      "21/09/26 05:40:53 INFO CodeGenerator: Code generated in 32.318716 ms\n",
      "+------+---------------+----------------+---------------+----------+----------+--------------+\n",
      "|연도별|서귀포시 인구수|65세이상 인구수 |14세이하 인구수|고령화비율|노령화지수|데이터기준일자|\n",
      "+------+---------------+----------------+---------------+----------+----------+--------------+\n",
      "|  2012|         154057|           25826|          22861|     16.76|    112.97|    2021-08-31|\n",
      "|  2013|         155641|           26936|          22393|     17.31|    120.29|    2021-08-31|\n",
      "|  2014|         158512|           27877|          22058|     17.59|    126.38|    2021-08-31|\n",
      "|  2015|         164519|           28979|          22362|     17.61|    129.59|    2021-08-31|\n",
      "|  2016|         170932|           30030|          23044|     17.57|    130.32|    2021-08-31|\n",
      "+------+---------------+----------------+---------------+----------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "21/09/26 05:40:53 INFO SparkUI: Stopped Spark web UI at http://172.30.1.27:4041\n",
      "21/09/26 05:40:53 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "21/09/26 05:40:53 INFO MemoryStore: MemoryStore cleared\n",
      "21/09/26 05:40:53 INFO BlockManager: BlockManager stopped\n",
      "21/09/26 05:40:53 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "21/09/26 05:40:53 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "21/09/26 05:40:53 INFO SparkContext: Successfully stopped SparkContext\n",
      "21/09/26 05:40:53 INFO ShutdownHookManager: Shutdown hook called\n",
      "21/09/26 05:40:53 INFO ShutdownHookManager: Deleting directory /private/var/folders/lr/jj1kxjwn315g3cjyb46358l80000gn/T/spark-bfa1460d-80fd-49ff-8904-e876df7fdd20/pyspark-54cf68b4-a885-4770-a2a4-4fa235e2ca94\n",
      "21/09/26 05:40:53 INFO ShutdownHookManager: Deleting directory /private/var/folders/lr/jj1kxjwn315g3cjyb46358l80000gn/T/spark-bfa1460d-80fd-49ff-8904-e876df7fdd20\n",
      "21/09/26 05:40:53 INFO ShutdownHookManager: Deleting directory /private/var/folders/lr/jj1kxjwn315g3cjyb46358l80000gn/T/spark-c63a6840-3dcd-408b-ba78-00e3fbb07808\n"
     ]
    }
   ],
   "source": [
    "!spark-submit src/ds3_popCsvRead.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
