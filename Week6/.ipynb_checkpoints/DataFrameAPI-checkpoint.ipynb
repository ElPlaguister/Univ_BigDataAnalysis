{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession.builder.master('local').appName('myApp').config(conf=pyspark.SparkConf()).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: string, value: string]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('set spark.sql.legacy.timeParserPolicy=LEGACY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType\n",
    "\n",
    "schema = StructType([])\n",
    "emptyDf = spark.createDataFrame(spark.sparkContext.emptyRDD(), schema)\n",
    "emptyDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## range : 일련의 수를 가진 DF 생성\n",
    "\n",
    "- 데이터프레임을 만들지 않고 함수를 실행해보기에 유용하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  2|\n",
      "|  4|\n",
      "|  6|\n",
      "|  8|\n",
      "+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "spark.range(0, 10, 2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|current_date()|\n",
      "+--------------+\n",
      "|    2021-10-11|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "spark.range(1).select(F.current_date()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|current_timestamp|\n",
      "+-----------------+\n",
      "|       1633900290|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(1).select(F.unix_timestamp().alias('current_timestamp')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1633900349"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.range(1).select(F.unix_timestamp().alias('current_timestamp')).rdd.collect()[0]['current_timestamp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## withColumn, Drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_c0', '_c1', '_c2']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tDf = spark.read.options(header='false', inferschema = 'true', delimiter ='\\t').csv(os.path.join('data', 'ds_spark_heightweight.txt'))\n",
    "tDf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- _c1: double (nullable = true)\n",
      " |-- _c2: double (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      " |-- weight: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tDf = tDf.withColumn('id', tDf._c0.cast('integer'))\n",
    "tDf = tDf.withColumn('height', tDf['_c1'].cast('double'))\n",
    "tDf = tDf.withColumn('weight', tDf['_c2'].cast('double'))\n",
    "tDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      " |-- weight: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tDf = tDf.drop('_c0').drop('_c1').drop('_c2')\n",
    "tDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1, height=65.78, weight=112.99),\n",
       " Row(id=2, height=71.52, weight=136.49)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tDf.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UDF : User Defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf = spark.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load(os.path.join('data', 'myDf.csv'))\n",
    "myDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------+------+\n",
      "|_c0|year|   name|height|\n",
      "+---+----+-------+------+\n",
      "|  0|   1|kim, js|   170|\n",
      "|  1|   1|lee, sm|   175|\n",
      "|  2|   2|lim, yg|   180|\n",
      "|  3|   2|    lee|   170|\n",
      "+---+----+-------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/11 06:25:01 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , year, name, height\n",
      " Schema: _c0, year, name, height\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/elplaguister/Workspace/Univ_BigDataAnalysis/Week6/data/myDf.csv\n"
     ]
    }
   ],
   "source": [
    "myDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HELLO WORLD'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def uppercase(s):\n",
    "    return s.upper()\n",
    "\n",
    "uppercase('hello World')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Column' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/lr/jj1kxjwn315g3cjyb46358l80000gn/T/ipykernel_36817/2984279153.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmyDf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'NAME'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muppercase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/lr/jj1kxjwn315g3cjyb46358l80000gn/T/ipykernel_36817/2470159258.py\u001b[0m in \u001b[0;36muppercase\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0muppercase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0muppercase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hello World'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Column' object is not callable"
     ]
    }
   ],
   "source": [
    "myDf = myDf.withColumn('NAME', uppercase(myDf.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n",
    "upperUDF = udf(uppercase, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDf = myDf.withColumn('NAM_E', upperUDF(myDf.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------+------+-------+\n",
      "|_c0|year|   name|height|  NAM_E|\n",
      "+---+----+-------+------+-------+\n",
      "|  0|   1|kim, js|   170|KIM, JS|\n",
      "|  1|   1|lee, sm|   175|LEE, SM|\n",
      "|  2|   2|lim, yg|   180|LIM, YG|\n",
      "|  3|   2|    lee|   170|    LEE|\n",
      "+---+----+-------+------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/11 06:25:33 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , year, name, height\n",
      " Schema: _c0, year, name, height\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/elplaguister/Workspace/Univ_BigDataAnalysis/Week6/data/myDf.csv\n"
     ]
    }
   ],
   "source": [
    "myDf.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Column' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/lr/jj1kxjwn315g3cjyb46358l80000gn/T/ipykernel_36817/2777518840.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmyDf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'Column' object is not callable"
     ]
    }
   ],
   "source": [
    "myDf['name'].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|kim, js|\n",
      "|lee, sm|\n",
      "|lim, yg|\n",
      "|    lee|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.select('name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_c0', 'int'),\n",
       " ('year', 'int'),\n",
       " ('name', 'string'),\n",
       " ('height', 'int'),\n",
       " ('NAM_E', 'string'),\n",
       " ('heightD', 'double')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "toDoublefunc = udf(lambda x: float(x), DoubleType())\n",
    "myDf = myDf.withColumn('heightD', toDoublefunc(myDf.height))\n",
    "myDf.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UDF함수로 조건에 따른 withColumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------+------+-------+-------+----------+\n",
      "|_c0|year|   name|height|  NAM_E|heightD|height>175|\n",
      "+---+----+-------+------+-------+-------+----------+\n",
      "|  0|   1|kim, js|   170|KIM, JS|  170.0|   shorter|\n",
      "|  1|   1|lee, sm|   175|LEE, SM|  175.0|    taller|\n",
      "|  2|   2|lim, yg|   180|LIM, YG|  180.0|    taller|\n",
      "|  3|   2|    lee|   170|    LEE|  170.0|   shorter|\n",
      "+---+----+-------+------+-------+-------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/11 06:29:29 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , year, name, height\n",
      " Schema: _c0, year, name, height\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/elplaguister/Workspace/Univ_BigDataAnalysis/Week6/data/myDf.csv\n"
     ]
    }
   ],
   "source": [
    "height_udf = udf(lambda x : 'taller' if x >= 175 else 'shorter', StringType())\n",
    "heightDf = myDf.withColumn('height>175', height_udf(myDf.height))\n",
    "heightDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 컬럼 명 변경 : withColumnRenamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n",
      "| ID|height|weight|\n",
      "+---+------+------+\n",
      "|  1| 65.78|112.99|\n",
      "|  2| 71.52|136.49|\n",
      "|  3|  69.4|153.03|\n",
      "+---+------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/11 08:42:16 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 931353 ms exceeds timeout 120000 ms\n",
      "21/10/11 08:42:16 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "tDf = tDf.withColumnRenamed('id', 'ID')\n",
    "tDf.show(3)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ebac036784c2d16c9d50bc630311939c724d93f9fd474f8cb81336a45de03920"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
